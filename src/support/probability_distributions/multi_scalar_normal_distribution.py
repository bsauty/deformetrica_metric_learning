from math import sqrt

import numpy as np
import torch
from torch.autograd import Variable

from support.utilities.general_settings import Settings


class MultiScalarNormalDistribution:
    ####################################################################################################################
    ### Constructor:
    ####################################################################################################################

    def __init__(self):
        self.mean = None
        self.variance_sqrt = None
        self.variance_inverse = None

    ####################################################################################################################
    ### Encapsulation methods:
    ####################################################################################################################

    def get_mean(self):
        return self.mean

    def set_mean(self, m):
        self.mean = m

    def get_variance_sqrt(self):
        return self.variance_sqrt

    def set_variance_sqrt(self, std):
        self.variance_sqrt = std
        self.variance_inverse = 1.0 / std ** 2

    def set_variance(self, var):
        self.variance_sqrt = sqrt(var)
        self.variance_inverse = 1.0 / var

    ####################################################################################################################
    ### Public methods:
    ####################################################################################################################

    def sample(self):
        return self.mean + self.variance_sqrt * np.random.standard_normal(self.mean.shape)

    def compute_log_likelihood(self, observation):
        """
        Fully numpy method.
        Returns only the part that includes the observation argument.
        """
        assert self.mean.size == 1 or self.mean.shape == observation.shape
        delta = observation.ravel() - self.mean.ravel()
        return - 0.5 * self.variance_inverse * np.sum(delta ** 2)

    def compute_log_likelihood_torch(self, observation):
        """
        Fully torch method.
        Returns only the part that includes the observation argument.
        """
        mean = Variable(torch.from_numpy(self.mean).type(Settings().tensor_scalar_type), requires_grad=False)
        assert mean.cpu().numpy().size == observation.cpu().numpy().size, \
            'mean.cpu().numpy().size = %d, \t observation.cpu().numpy().size = %d' \
            % (mean.cpu().numpy().size, observation.cpu().numpy().size)
        delta = observation.contiguous().view(-1, 1) - mean.contiguous().view(-1, 1)
        return - 0.5 * torch.sum(delta ** 2) * self.variance_inverse
